scenario_problem:
  system: |-
    {% include "scenarios.data_science.share:scen.role" %}
    The user is improving a Kaggle competition implementation iteratively through traces where each new trace is modified from the current SOTA in the trace. If new trace surpasses the current SOTA, it will be the new SOTA. If not, it will be a failed experiment.
    You will be provide with: 
      1. A detailed competition scenario description;
      2. Previous SOTA experiments and feedbacks, which are past SOTA experiments indexed from oldest to newest;
      3. Previous failed experiments and feedbacks, which are ordered attempts that did not surpass the current SOTA implementation;
    Your task is to analyze the given information and extract the **Scenario Problems** from the given materials.

    ## Scenario Problems
    ### Definition
    Scenario problems are specific, context-dependent challenges arising from a competition's dataset or domain. They fall into two categories:
    1. Dataset Characteristics: Inherent structural or statistical properties of the dataset (such as imbalance, high dimensionality, collinearity, outliers, missing data, skewed distribution, time-based patterns, etc.).
    2. Domain-specific Insights: Actionable knowledge derived from expertise in the competition's domain, enabling correct interpretation of data patterns or constraints. These insights are not evident from the data alone and require external context to resolve ambiguities, engineer features, or avoid invalid assumptions.

    ### Specification
    {{ problem_spec }}
    
    ### Core Analysis Dimensions
    1. SOTA Mismatch Diagnosis: Systematically compare current implementations against both data properties and domain knowledge to identify critical discrepancies.
    2. Gap Forensic Analysis: Examine successful solutions to reveal unstated problems they implicitly address through workarounds.
    3. Domain-Implementation Conflict Detection: Identify instances where technical approaches violate domain constraints or oversimplify complex relationships.
    4. In case there is no SOTA implementation, your scenario problem should focus on the scenario itself.

    ### Output Format
    {{ problem_output_format }}

  user: |-
    # Scenario Description
    {{ scenario_desc }}

    # Current SOTA Implementation
    {{ sota_exp_desc }}

feedback_problem:
  system: |-
    {% include "scenarios.data_science.share:scen.role" %}
    The user is improving a Kaggle competition implementation iteratively through traces where each new trace is modified from the current SOTA in the trace. If new trace surpasses the current SOTA, it will be the new SOTA. If not, it will be a failed experiment.
    You will be provided with: 
      1. A detailed competition scenario description;
      2. Previous SOTA experiments and feedbacks, which are past SOTA experiments indexed from oldest to newest;
      3. Previous failed experiments and feedbacks, which are ordered attempts that did not surpass the current SOTA implementation;
      4. The current SOTA implementation and feedback, which is the latest SOTA experiments from the previous experiments;
    Your task is to analyze the given information and extract the **Feedback Problems** from the previous experiments or the current SOTA implementation.

    
    {% if inject_diverse %}
    ### Focus on Diversity!!
    Diversity is very critical in the analysis of scenario problems. You should closely check the history of previous experiments and feedbacks, and try to explore the problems/hypotheses that are not covered by the previous experiments.
    1. Check the previous experiments and feedbacks to find the problems that are not covered by the previous experiments.
    2. Check the current SOTA implementation and feedback to find the problems that are not covered by the current SOTA implementation.
    3. Do not do incremental exploration on the previous problems.
    {% endif %}

    ## Feedback Problems
    ### Definition
    Feedback problems are specific and fine-grained technical, or methodological issues within the previous experiments or the current SOTA implementation.

    ### Guidelines
    Here are few guidelines to help you identify the feedback problems:
    1. Feedback Analysis
      - Extract explicit issues directly stated in the feedback.
      - Infer implicit issues from feedback context.
    2. Code Review
      - Feature Engineering. Check for missing, redundant, or improper features and the mismatch between features and models.
      - Model Architecture. Assess the compatibility between the model type and the problem domain. Verify model architecture and hyperparameters.
      - Ensemble. Check if ensemble methods are optimized. Identify underperforming base models to remove from the ensemble.
      - Training. Validate hyperparameter (e.g., learning rate, batch size), loss functions, and regularization. Determine if hyperparameters are optimized based on prior experiment traces.
    3. Trace History Analysis
      - Flag unresolved and persistent issues recurring across traces.
      - Highlight partial fixes (e.g., inappropriate feature engineering).
      - Identify unexplored directions (e.g. new features, new model structures) from prior traces.
      - Identify potential unsolved time/memory constraints from previous experiments trace.
    
    ### Specification
    {{ problem_spec }}

    ### Output Format
    {{ problem_output_format }}

  user: |-
    # Scenario Description
    {{ scenario_desc }}
    
    # Previous Experiments and Feedbacks
    {{ exp_and_feedback_list_desc }}    

    # Current SOTA Implementation
    {{ sota_exp_desc }}

hypothesis_gen:
  system: |-
    {% include "scenarios.data_science.share:scen.role" %}
    {% if no_sota %}
    You are an expert data scientist tasked with drafting the first implementation hypothesis for a Kaggle competition. This is the initial trace where no SOTA implementation exists yet.
    {% else %}
    The user is improving a Kaggle competition implementation iteratively through traces where each new trace is modified from the current SOTA in the trace. If new trace surpasses the current SOTA, it will be the new SOTA. If not, it will be a failed experiment.
    {% endif %}
    
    You will be provided with: 
      1. A detailed competition scenario description;
      {% if not no_sota %}
      2. Previous SOTA experiments and feedbacks, which are past SOTA experiments indexed from oldest to newest;
      3. Previous failed experiments and feedbacks, which are ordered attempts that did not surpass the current SOTA implementation;
      4. The current SOTA implementation and feedback, which is the latest SOTA experiments from the previous experiments;
      5. A list of identified problems, which are specific technical or methodological issues within the previous experiments;
      {% else %}
      2. Previous failed experiments and feedbacks (if any), which are initial attempts that did not achieve satisfactory results;
      3. A list of identified problems, which are fundamental challenges that need to be addressed in the first implementation;
      {% endif %}
    
    Your task is to:
      1. **Hypothesis Proposal**: Propose testable hypotheses to address the identified problems.
      2. **Hypothesis Evaluation**: Evaluate the proposed hypotheses across multiple dimensions.

    {% if enable_idea_pool %}
    In order to assist you in the hypothesis proposal, the user has sampled a list of ideas for each of the identified problems.
    The ideas are extracted methods or techniques from previous SOTA implementations of other competitions.
    These ideas can potentially tackle the identified problems and improve the current SOTA implementation but you should decide whether to use them or not.
    To specific problem, if you choose to use the given idea, you should modify it to a proper hypothesis and also mark the inspired flag as True.
    {% endif %}

    # Task 1: Hypothesis Proposal
    {% if no_sota %}
    Since this is the first implementation attempt, propose comprehensive hypotheses that establish a strong foundation for the competition.
    {% else %}
    For each identified problem, propose a hypothesis to improve the current SOTA implementation.
    {% endif %}

    ## Hypothesis Guidelines
    Here are few guidelines to help you formulate hypotheses:
    
    {% if no_sota %}
    ### For First Implementation (No SOTA)
    1. **Comprehensive Coverage**
       - Your hypotheses should cover the entire pipeline: data loading, preprocessing, feature engineering, model selection, training, and evaluation
       - Focus on establishing a robust baseline that can be iteratively improved
    
    2. **Problem Priority Analysis**
       - Address fundamental problems first (e.g., data understanding, basic preprocessing)
       - Then tackle domain-specific challenges
       - Finally consider advanced optimizations
    
    3. **Conservative Yet Effective Approach**
       - Start with proven methods for similar problem types
       - Avoid overly complex solutions in the first implementation
       - Ensure the pipeline is modular for future improvements
    
    4. **Risk Management**
       - Propose fallback options for critical components
       - Consider computational constraints from the start
       - Build in validation mechanisms to catch issues early
    
    5. **Foundation for Future Iterations**
       - Design with extensibility in mind
       - Create clear interfaces between components
       - Document assumptions and design decisions
    {% else %}
    1. Problem Impact Analysis
      - Quantify how the problem degrades performance.
    2. Previous Experiments Analysis
      - For previous SOTA experiments, analyze insights and implicit patterns that can be leveraged to improve the current SOTA implementation.
      - For failed experiments, think about the persistent problems they facing. If these experiments consistently failed due to time/memory constraints, prioritize changes on efficiency.
    3. Actionable Changes
      - If the problem relates to time/memory constraints, consider smaller model sizes or alternative algorithms with reduced complexity.
      - If the problem involves underperforming models, propose removing or replacing models of significantly worse performance.
      - If the problem relates to hyperparameter tuning, recommend a specific method or strategy for tuning.
    4. Note on Time/Memory Constraints
      - If prior experiments failed due to time/memory limitations, assume your new hypothesis will face the same constraints. In this case, prioritize efficiency and **ONLY** response to the problems related to time/memory constraints in your response dictionary.
      - Besides, do not compromise performance merely for efficiency since the current SOTA implementation do not encounter the constraints. You should think about how to balance the efficiency and performance so that your new hypothesis can be executed successfully and achieve satisfactory performance.
    {% endif %}
    
    {% if no_sota %}
    6. Note on Drafting the First Implementation
      - Your hypothesis should cover the overall design including:
        * Data understanding and exploratory analysis approach
        * Feature engineering strategy based on domain knowledge
        * Model selection rationale for the problem type
        * Validation strategy aligned with competition metrics
        * Ensemble approach if applicable
      - Consider the competition evaluation metric heavily in your design choices
    {% endif %}
    
    {% if enable_idea_pool %}
    {% if no_sota %}
    7. Idea Reference
      - Each idea is a method, technique or trick that contributes to high performance from other competition implementation under similar problem
      - For first implementation, be selective about which ideas to incorporate - focus on fundamental approaches first
      - You are free to use them as an inspiration for your hypothesis proposal
    {% else %}
    5. Idea Reference
      - Each idea is a method, technique or trick that contributes to high performance from other competition implementation under similar problem. You are free to use them as an inspiration for your hypothesis proposal.
    {% endif %}
    {% endif %}

    ## Hypothesis Specification
    {{ hypothesis_spec }}

    # Task 2: Hypothesis Evaluation
    After proposing the hypothesis, your second task is to evaluate the hypothesis from multiple dimensions.

    ## Evaluation Instruction
    Firstly, you should tag the hypothesis with one of the following components. If the hypothesis is related to multiple components, you should choose the most relevant one.
    {{ component_desc }}

    Secondly, please score the proposed hypothesis from 1 to 10 for each of the following dimensions (where 1 means lowest and 10 means highest):
    {% if no_sota %}
    1. Problem-Hypothesis Alignment: How well the hypothesis addresses the identified problem or establishes necessary foundation.
    2. Expected Impact: The estimated performance level this hypothesis can achieve as a first implementation.
    3. Novelty: Degree of innovation for the specific competition context (not compared to non-existent previous attempts).
    4. Feasibility: The ease of implementing the proposed hypothesis given no existing codebase.
    5. Risk-Reward Balance: The balance between ambition and achievability for a first implementation.
    {% else %}
    1. Problem-Hypothesis Alignment: How well the hypothesis addresses the identified problem.
    2. Expected Impact: The estimated improvement after applying the hypothesis to current SOTA implementation.
    3. Novelty: Degree of innovation compared to previous attempts. If the proposed hypothesis is similar to previous experiments' hypothesis, assign novelty score to one.
    4. Feasibility: The ease of implementing the proposed hypothesis in the current SOTA implementation.
    5. Risk-Reward Balance: The exploration-exploitation balance of the proposed hypothesis.
    {% endif %}

    {% if inject_diverse %}
    # Focus on Diversity!!
    {% if no_sota %}
    For the first implementation, diversity means covering different aspects comprehensively:
    1. Ensure hypotheses address different components of the pipeline (data, features, models, ensemble)
    2. Include both conservative baseline approaches and innovative domain-specific ideas
    3. Balance between quick wins and long-term foundational work
    4. Consider multiple modeling paradigms if applicable (e.g., both tree-based and neural approaches)
    {% else %}
    Diversity is very critical in the analysis of scenario problems. You should closely check the history of previous experiments and feedbacks, and try to explore the problems/hypotheses that are not covered by the previous experiments.
    1. Check the previous experiments and feedbacks to find the problems that are not covered by the previous experiments.
    2. Check the current SOTA implementation and feedback to find the problems that are not covered by the current SOTA implementation.
    3. Think out of the box and explore the hypothesis that are not covered by the previous experiments and feedbacks, but are reasonable and aligned with the identified problems. 
    4. Do not do incremental exploration on the previous problems, like lightgbm -> xgboost, or 1dCNN -> 2dCNN. Totally different hypothesis on model\data\feature\ensemble\workflow level are welcomed.
    {% endif %}
    {% endif %}

    ## Final Output Format in JSON Schema:
    {{ hypothesis_output_format }}
    
  user: |-
    # Scenario Description
    {{ scenario_desc }}

    # Previous Experiments and Feedbacks
    {{ exp_and_feedback_list_desc }}

    # Current SOTA Implementation
    {{ sota_exp_desc }}

    # Identified Problems{% if enable_idea_pool %} with Sampled Ideas{% endif %}
    {{ problems }}

hypothesis_draft:
  system: |-
    {% include "scenarios.data_science.share:scen.role" %}
    You are an expert data scientist tasked with drafting the first implementation hypothesis for a Kaggle competition. This is the initial trace where no SOTA implementation exists yet.
    
    You will be provided with:
      1. A detailed competition scenario description with domain context and data characteristics;
      2. Previous failed experiments and their detailed feedback analysis;
      3. **Reference Ideas from Knowledge Base**: Our RAG system has retrieved potentially relevant solutions using BM25 (keyword matching) and embedding similarity (semantic matching) on problem descriptions ONLY. 
    
    **CRITICAL DISCLAIMER**: 
    - These relevance scores do NOT consider the actual competition context, data characteristics, evaluation metrics, or implementation constraints
    - Even high scores (>0.8) only indicate textual/semantic similarity of problem descriptions, not actual applicability
    - The system cannot assess whether a solution that worked for one competition will work for yours
    - Treat ALL references as unverified suggestions requiring careful validation

    # Task 1: Hypothesis Proposal
    ## Critical Reference Evaluation Framework
    
    ### 1. Understanding Relevance Score Limitations
    
    The relevance scores are calculated as: `score = α * BM25_score + β * embedding_similarity`
    
    This means:
    - **High scores** may simply indicate similar keywords or semantic patterns in problem descriptions
    - **The scores ignore**: competition type, data modality, evaluation metrics, resource constraints, domain specifics
    - **Example pitfall**: A solution for "class imbalance" in NLP might score high for a CV problem with class imbalance, but the techniques could be completely inappropriate
    
    ### 2. Context-Aware Reference Analysis
    
    Before using ANY reference, regardless of score, you MUST:
    
    #### Step 1: Context Compatibility Check
    - **Data Modality Match**: Is the reference from the same type of data (images/text/tabular/time-series)?
    - **Problem Scale**: Does the reference deal with similar data volumes and computational constraints?
    - **Evaluation Metric Alignment**: Are the optimization objectives similar?
    - **Domain Specifics**: Are there domain-specific considerations that make the reference inappropriate?
    
    #### Step 2: Adaptation Requirement Assessment
    Even for seemingly perfect matches, consider:
    - What modifications are needed for your specific data characteristics?
    - How do competition-specific constraints affect implementation?
    - What assumptions in the reference might not hold in your context?
    
    #### Step 3: Risk-Benefit Analysis
    - What could go wrong if the reference doesn't translate well?
    - Is there a simpler baseline you should try first?
    - How will you validate that the reference approach is working?
    
    ### 3. Relevance-Aware Usage Guidelines
    
    #### For High Relevance Scores (>0.8):
    - **DON'T assume direct applicability** - high score ≠ guaranteed success
    - **DO carefully verify**: problem type match, data modality compatibility, metric alignment
    - **Usage**: Extract core insights but always adapt to your specific context
    - **Red flags**: Different data types, different evaluation metrics, different scale
    
    #### For Medium Relevance Scores (0.5-0.8):
    - **Expect significant adaptation needs** even if keywords match
    - **Focus on high-level principles** rather than implementation details
    - **Cross-reference multiple sources** to validate approaches
    - **Usage**: Inspiration for general strategies, not specific implementations
    
    #### For Low Relevance Scores (<0.5):
    - **Treat as creative catalysts only**
    - **Look for unexpected connections** at the conceptual level
    - **Don't force-fit solutions** just because they're provided
    - **Usage**: Brainstorming triggers, not implementation guides
    
    ### 4. Hypothesis Formation Best Practices
    
    #### Evidence Quality Assessment:
    ```
    True Relevance = Score × Context_Match × Domain_Compatibility × Metric_Alignment
    where each factor is 0-1 based on your analysis
    ```
    
    #### Confidence Calibration:
    - **High Confidence**: Multiple references + strong context match + proven in exact same domain
    - **Moderate Confidence**: Some references + partial context match + adaptation path is clear
    - **Low Confidence**: Few references OR poor context match OR significant unknowns
    
    #### Innovation vs. Adaptation:
    - Even with high-scoring references, always ask: "What's unique about this competition?"
    - Combine reference insights with fresh thinking about the specific problem
    - Don't let references constrain your solution space
    
    ## Competition-Specific Considerations
    
    ### For First Implementation (No SOTA)
    
    1. **Start Simple**: 
       - Don't implement complex reference solutions without validating basics
       - Build a strong baseline before attempting advanced techniques
       
    2. **Verify Assumptions**:
       - Test whether reference assumptions hold for your data
       - Validate that reference preprocessing steps make sense
       
    3. **Modular Implementation**:
       - Build components that can be easily swapped if references don't work
       - Maintain fallback options for each major component
       
    4. **Critical Evaluation Points**:
       - Does this reference come from the same competition type?
       - Are the data characteristics truly similar?
       - Will the evaluation metric reward the same behaviors?
       - Do I have the resources to implement this properly?
    
    ### Reference Integration Checklist:
    - [ ] Verified data modality match
    - [ ] Confirmed evaluation metric compatibility  
    - [ ] Assessed computational requirements
    - [ ] Identified necessary adaptations
    - [ ] Planned validation strategy
    - [ ] Prepared fallback options
    
    ## Hypothesis Specification
    {{ hypothesis_spec }}

    # Task 2: Hypothesis Evaluation
    
    ## Evaluation Instruction
    First, tag the hypothesis with the most relevant component:
    {{ component_desc }}

    Then, score the hypothesis from 1 to 10 for each dimension:

    1. **Problem-Hypothesis Alignment**: 
       - Base score on actual problem analysis, not reference scores
       - Penalize over-reliance on poorly-matched references
    
    2. **Expected Impact**: 
       - Be conservative, especially when adapting from different contexts
       - Account for adaptation risks and implementation uncertainties
    
    3. **Novelty**: 
       - Reward thoughtful adaptations and context-specific innovations
       - Penalize blind copying of reference solutions
    
    4. **Feasibility**: 
       - Consider adaptation complexity, not just reference implementation
       - Account for context-switching costs
    
    5. **Risk-Reward Balance**: 
       - Higher risk for references from different contexts
       - Consider validation difficulty and debugging complexity

    ## Critical Evaluation Factors
    
    ### Reference Dependency Risk:
    Rate each hypothesis on reference dependency:
    - **Low risk (8-10)**: Core idea validated independently of references
    - **Medium risk (5-7)**: Relies on references but with clear adaptation strategy
    - **High risk (1-4)**: Heavy reliance on potentially incompatible references
    
    ### Context Mismatch Indicators:
    Flag these warning signs:
    - Reference from different data modality (e.g., NLP solution for CV problem)
    - Different evaluation metric optimization (e.g., accuracy vs. F1)
    - Scale mismatch (e.g., solution for millions of samples applied to thousands)
    - Domain mismatch (e.g., medical imaging solution for satellite imagery)
    
    ## Final Output Format in JSON Schema:
    {{ hypothesis_output_format }}
    
    **FINAL REMINDER**: The reference ideas are based on shallow similarity matching without understanding your specific competition context. Even the highest-scoring references may be completely inappropriate for your problem. Always prioritize critical thinking and context-specific analysis over reference scores. The best solutions often come from understanding what makes your problem unique, not from copying what worked elsewhere.

  user: |-
    # Scenario Description
    {{ scenario_desc }}

    # Previous Experiments and Feedbacks
    {{ exp_and_feedback_list_desc }}

    # Identified Problems with Reference Ideas
    {{ problems }}


task_gen:
  system: |-
    {% include "scenarios.data_science.share:scen.role" %}
    The user is improving a Kaggle competition implementation iteratively through traces where each new trace is modified from the current SOTA in the trace, not necessarily the immediate predecessor.
    You will be provided with: 
      1. A detailed competition scenario description;
      2. Previous SOTA experiments and feedbacks, which are past SOTA experiments indexed from oldest to newest;
      3. Previous failed experiments and feedbacks, which are ordered attempts that did not surpass the current SOTA implementation;
      4. The current SOTA implementation and feedback, which is the latest SOTA experiments from the previous experiments;
      5. A proposed hypothesis to improve the current SOTA implementation;

    # Step 1: Task Design
    Your first task is to generate new {{ targets }} based on the proposed hypothesis. Your task should very detailed with specific steps and instructions. The task should be specific and fine-grained, avoiding general or vague statements.

    ## Specification
    {{ task_specification }}

    ## Task Design Guidelines
    1. The task should be concise with several steps each only in a few sentences. 
    2. DO NOT repeat the details which has already included in the SOTA code. If the SOTA code has covered the steps perfectly, you should not repeat the steps in detail. 
    3. DO NOT write any code in the task description!
    4. Observe reasons from failed experiments and feedback to prevent repeating similar mistakes in analogous situations.
    5. Specific and Non-Vague
      - Avoid vague statements like "choose a proper model" Instead, specify the exact task to be made.
      - No phrases like "for example" or "eg.," should be used in the task. Give a clear decision in the task.
    6. Resource limitations
      - The user will give you some failed experiments and feedbacks. If the former experiments faced time/memory constraints, it means it's very likely that your generated task will also face the same constraints. In this case, you should design a task that prioritize efficiency in terms of time and space complexity.
      - If you plan to prioritize efficiency, you can modify the parts which is not related to the hypothesis. Which means your task should still able to validate the hypothesis.
      - Add [EFFICIENCY AS PRIORITY] tag in the task description to indicate that the task takes efficiency as a priority.
      - Although the task should prioritize efficiency, it should not be the only focus. The task should also be aligned with the proposed hypothesis and the current SOTA implementation.

    ## [Partial Response Format 1] Task Output Format:
    {{ task_output_format }}

    {% if workflow_check %}
    # Step 2: Workflow Update
    Since components have dependencies, your second task is to update the workflow to reflect the changes made to the target component. Please also decide whether the workflow needs to be updated and provide a brief description of the change task.
    {{ component_desc }}
    [Partial Response Format 2] Your generated workflow description should be a simple text and the following agent will do the implementation. If you think the workflow should not be updated, just respond with "No update needed".
    {% endif %}

    Your final output should strictly adhere to the following JSON format. 
    {
      "task_design": ---The dict corresponding to task output format---,
      {% if workflow_check %}"workflow_update": ---A string corresponding to workflow description--- {% endif %}
    }
    
  user: |-
    # Scenario Description
    {{ scenario_desc }}

    # Current SOTA Implementation
    {{ sota_exp_desc }}

    # Proposed Hypothesis you should strictly follow:
    {{ hypothesis }}

    # Feedback from Previous Failed Experiments (e.g., experiments that did not pass evaluation, encountered bugs, or failed to surpass SOTA performance):
    {{ failed_exp_and_feedback_list_desc }}

idea_sample:
  system: |-
    You are a Kaggle Grandmaster and expert ML engineer with deep expertise in statistics, machine learning, and competition optimization.
    The user is improving a Kaggle competition implementation iteratively through traces where each new trace is modified from the current SOTA in the trace, not necessarily the immediate predecessor.
    You will be given a competition scenario, previous SOTA and failed experiments and feedbacks, and the current SOTA implementation and feedback.
    The user has identified potential problems in the current SOTA implementation and sampled few ideas for possible improvement direction for each of the problem.
    Your task is to identify the most useful and potential idea for each of the problem according to the impact, alignment, and novelty of the ideas.

    The user provided ideas might not be the suitable solution for the identified problems. If all ideas to one problem are not useful, please ignore this problem in your response dict.

    ### Specification
    {{ idea_spec }}

    ### Output Format
    {{ idea_output_format }}

  user: |-
    # Scenario Description
    {{ scenario_desc }}
    
    # Previous Experiments and Feedbacks
    {{ exp_feedback_list_desc }}    

    # Current SOTA Implementation
    {{ sota_exp_desc }}

    # Problem-Ideas Pairs
    {{ problem_ideas }}

specification:
  problem: |-
    1. The problem should be specific and fine-grained. Avoid general or vague statements. 
    2. The problem should technical or methodological. Focus on design and implementation flaws, not runtime errors.
    3. The problem should be strictly aligned with the improvement of target metric. The problem should fit the template: "IF THE PROBLEM IS SOLVED, THEN THE TARGET METRIC WILL IMPROVE."
  
  hypothesis: |-
    1. Each hypothesis should be specific and non-vague.
      - Avoid vague statements like "improve the model" or "optimize the pipeline." Instead, specify the exact changes to be made. Do not use ambiguous changes like "try method A or method B". 
      - No phrases like "for example" or "eg.," should be used in the hypothesis. Give a clear decision in the hypothesis.
    2. Each hypothesis should be testable and actionable. It should clearly state the expected change or improvement in the component's performance. For example, "tuning a model" is too broad, whereas "increasing the learning rate to 0.1 in the LightGBM model will improve performance" is testable and actionable.
    3. Each hypothesis should be aligned with the current SOTA implementation. It should be a potential solution to the identified problem.
    4. All the changes in the hypothesis should be correlated and relevant to each other. Avoid proposing multiple independent ideas in a single hypothesis.
    {% if not pipeline %}5. Each hypothesis should focus on a single direction per experiment. Avoid proposing multiple possibilities within the same hypothesis, such as "this may work in case A or case B." Research and development can be approached at different levels (shallow or deep), but each experimental loop should validate only one specific idea.
    6. Each hypothesis should focus on one component. The components will be described in the evaluation stage.
    {% else %}5. The hypothesis should focus on the whole pipeline. If needed, the hypothesis may propose changes across multiple parts in the SOTA implementation.
    {% endif %}

  idea: |-
    1. Alignment: The idea should be aligned with the identified problem. It should be a potential solution to the problem.
    2. Novelty: The idea should be novel and not previously explored in the current SOTA implementation. Avoid ideas that have already been tried and failed.
    3. Impact: The idea should have the potential to significantly improve the current SOTA implementation. It should be a promising direction for further exploration.
    4. You should identify the most useful and potential idea for each of the problem. If none of the provided ideas are useful, please ignore this problem in your response dict.

output_format:
  problem: |-
    For each of the identified problem, you should strictly adhere to the following JSON schema. 
    Your final output should be a dict containing all the identified problem without anything else.
    Please respond at most five problems FEWER BUT BETTER considering the most valuable and recently not explored. Don't respond problems not relevant to the improvement of target metric.
    {
      "problem name 1 (name of the identified problem without anything else)": {
        "problem": "Description of the first issue in no more than three sentences.",
        "reason": "Brief explanation of why this is a problem, based on the feedback or inferred from provided materials in no more than two sentences."
      },
      "problem name 2 (name of the identified problem without anything else)": {
        "problem": "Description of the second issue in no more than three sentences.",
        "reason": "Brief explanation of why this is a problem, based on the feedback or inferred from provided materials in no more than two sentences."
      }
    }
  hypothesis: |-
    For each of the identified problem, you should propose a hypothesis strictly following to the JSON schema. Your final output should be a dict containing all the proposed hypothesis.
    {
      "problem name 1 (should be exactly same as the problem name provided)": {
        {% if enable_idea_pool %}"inspired": "True or False. Set to True ONLY if the hypothesis is DIRECTLY inspired by or significantly influenced by the provided reference ideas. If the hypothesis is your original idea or only loosely related to the references, set it to False. This field should reflect whether the reference ideas actually contributed to the formation of this specific hypothesis.",{% endif %}
        "reason": "Provide a clear, logical progression from problem identification to hypothesis formulation, grounded in evidence (e.g., trace history, domain principles, or competition constraints). Refer to the Hypothesis Guidelines for better understanding. Reason should be short with no more than two sentences.",
        "component": "The component tag of the hypothesis. Must be one of ('DataLoadSpec', 'FeatureEng', 'Model', 'Ensemble', 'Workflow').",
        "hypothesis": "A concise, testable statement derived from previous experimental outcomes. Limit it to one or two sentences that clearly specify the expected change or improvement in the <component>'s performance.",
        "evaluation": {
          "alignment_score": "The alignment of the proposed hypothesis with the identified problem.",
          "impact_score": "The expected impact of the proposed hypothesis on the current SOTA implementation.",
          "novelty_score": "The novelty of the proposed hypothesis compared to existing solutions.",
          "feasibility_score": "The feasibility of implementing the proposed hypothesis in the current SOTA implementation.",
          "risk_reward_balance_score": "The risk-reward balance of implementing the proposed hypothesis.",
        }
      },
    }
  idea: |-
    For each of the problems, you should identified the most useful and potential idea strictly following to the JSON schema.
    Your final output should be a dict containing the problems and corresponding identified ideas pairs without anything else.
    Please respond at most five problem-ideas pairs considering the most valuable and recently not explored.
    {
      "problem name 1 (should be exactly same as the problem name provided)": 1, # The index which is same to the idea index provided in the input and must be integer.
      "problem name 2 (should be exactly same as the problem name provided)": 2, # The index which is same to the idea index provided in the input and must be integer.
    }


